<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Better than a Coin Toss</title>

		<meta name="description" content="">
		<meta name="author" content="Richard Warburton">
		<meta name="author" content="John Oliver">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.js/css/reveal.min.css">
		<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		<div class="reveal">
			<div class="slides">

				<section>
					<h1>Are you better than a Coin Toss?</h1>
					<h3>by John Oliver and Richard Warburton</h3>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>

				</section>

				<section>
                    <img src="imgs/mad_men.jpg" />

					<aside class="notes">
                        <br /> Display Advertising
                        <br /> Complex internal algorithm
                        <br /> Baselined by an expert found to be no better than randomly selecting adverts
					</aside>

				</section>
				
                <section>
                    <h2>Who are we?</h2>

                    <aside class="notes">
                        <br /> John first
                        <br /> Working on Automated Performance Tuning Problems
                        <br /> based on science
                        <br /> ML at work
					</aside>

				</section>
                
                <section>
                    <ul style="font-size: 48px; line-height:2">
                        <li>Why you should care</li>
                        <li>The Fundamentals</li>
                        <li>Practical Problems</li>
                        <li>Applying the Theory</li>
                    </ul>
				</section>

                <section>
                    <h2>'Experts" aren't very good</h2>
                    
                    <img src="imgs/Daniel_KAHNEMAN.jpg" />
                    
                    <aside class="notes">
                        <br /> Kahneman
                        <br /> Nobel Prize Winner
                        <br /> Prospect Theory - explain poor decisions when risk is a factor
					</aside>

				</section>
                
                <section>
                    <aside class="notes">
                        Sports
					</aside>
				</section>
                
                <section>
                    <aside class="notes">
                        meterology
                        hurricane
                        http://fivethirtyeight.blogs.nytimes.com/2012/09/09/why-weather-forecasters-are-role-models/
					</aside>
				</section>
                
                <section>
                    <aside class="notes">
                        finance
					</aside>
				</section>
                
                <section>

                    <img src="imgs/tetlock.gif" style="float: left;"/>
                    <img src="imgs/nate-silver.jpg" />
                    
                    <aside class="notes">
                        Politics:
                            2008 and 2012 elections had predictable results.
                            Nate Silver combined individual poll ratings.
                            Dismissed by critics, data proved him right.
					</aside>
				</section>

                <section>
                    <h1>Big Data solves ALL KNOWN PROBLEMS</h1>
				</section>

                <section>
                    <h1>Big Data <strike>solves ALL KNOWN PROBLEMS</strike></h1>
                    <h3> ... helps </h3>
                    <aside class="notes">
                        <br />recording data not enough
                        <br />need to know how to analyse it
                        <br />New problem: validation
					</aside>
				</section>

                <section>
                    <h1>Validation = Tests for Data</h1>
                    <aside class="notes">
                        TODO: pithy line/quote
                        explain validation
                        You wouldnâ€™t trust software without testing it - how do you trust your numbers?
					</aside>
				</section>

                <section>
                    <aside class="notes">
                    </aside>
                </section>
		
                <section>
                    <h1>Part 1: Fundamentals</h1>
                    <aside class="notes">
                        Talk about fundamentals, by which we mean basic statistics. <br />
                        Probably familiar to some, but important to cover fundamentals.
                    </aside>
                </section>
                
                <section>
                    <h2>Null Hypothesis</h2>
                    <p>Until proven otherwise there is no relationship between phenomena</p>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h4>Hypothesis: the loss of my socks is due to alien burglary</h4>
                    <h4>Null Hypothesis: the loss of my socks is nothing to do with alien burglary</h4>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h4>Refuting The Null Hypothesis is to present evidence that there is a relationship</h4>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h3>When you hear "Wolf!" there is a wolf nearby</h3>
                    <br />
                    <table>
                        <tr>
                            <td></td>
                            <th>Cry "Wolf!"</th>
                            <th>Stay Quiet</th>
                        </tr>
                        <tr>
                            <th>Wolf Nearby</th>
                            <td>Ok</td>
                            <td>False Negative</td>
                        </tr>
                        <tr>
                            <th>Its really a chicken!</th>
                            <td>False Positive</td>
                            <td>Ok</td>
                        </tr>
                    </table>
                    <br />
                    <br />
                    <br />
                    <aside class="notes">
                        <h4>False Positives vs False Negatives</h4>
                        FN or type I error - when the null hypothesis (H0) is true, but is rejected <br />
                        FP or type II error - when the null hypothesis (H0) is false, but is accepted
                    </aside>
                </section>

                <section>
                    <h2>Why is this important?</h2>
                    <aside class="notes">
                        FP and FN not equal <br />
                        Depend on business needs and context. <br />
                        Different people value them differently.
                    </aside>
                </section>

                <section>
                    <blockquote>It is better that ten guilty persons escape than that one innocent suffer</blockquote>
                    <p>- William Blackstone</p>
                    <aside class="notes">
                        Innocent until proven guilty <br />
                        The law considers FP more harmful than FN.
                    </aside>
                </section>

                <section>
                    <img src="imgs/sleazy-salesman.jpg" />
                    <aside class="notes">
                        Estate Agents would rather show people the wrong house than miss out on a sale.
                    </aside>
                </section>
               
               <!-- TODO: more logs -->
                <section>
                    <h2>Static Analysis</h2>
                    <img src="imgs/buggy-sm.png" />
                    <img src="imgs/eclipse3.2.gif" />
                    <aside class="notes">
                        Coverity published research findings that low false positive rates are good for business. <br />
                        If you tell developers there's a bug and there isn't one - they lose confidence in your tooling.
                    </aside>
                </section>

                <section>
                    <h2>Cost benefit Analysis</h2>
                    <ul>
                        <li>Costs a lot to jail an innocent man
                        <li>Costs very little to show someone an inappropriate house
                        <li>Credibility, Liberty, Morality are also costs
                    </ul>
                </section>

                <section>
                    <h2>Choose the right measurement</h2>
                    <aside class="notes">
                        Most people think of 'accuracy' <br />
                        Accuracy: what you got right / everything <br />
                        Not an appropriate measurement for all situations.
                    </aside>
                </section>

                <section>
                    <h2>Recall</h2>
                    <p>number of true positives / number of actually true values</p>
                    <aside class="notes">
                        Proportion of true values you've found. <br />
                        How how much did you get right?<br />
                        Low score = too many FNs.<br />
                        Irrational Skeptic (Richard's Dad)
                    </aside>
                </section>

                <section>
                    <h2>Precision</h2>
                    <p>number of true positives / predicted true value</p>
                    <aside class="notes">
                        Proportion of your claimed truths which were true. <br />
                        Low score = too many FPs.<br />
                        Irrational Optimist (Dot Com Boom)
                    </aside>
                </section>

                <section>
                    <h2>F Measure</h2>
                    <!-- TODO: fix image -->
                    <img src="imgs/fmeasure.png" />
                    <aside class="notes">
                        Weighted tradeoff between precision and recall. <br />
                        We need to have a balance between each. <br />
                        Won't explain formula, go read about it. <br />
                        F1 = even tradeoff between precision and recall.
                    </aside>
                </section>

                <section>
                    <h2>Case Study: Memory Leaks</h2>
                    <br />
                    <p>About ~10% of our dataset had memory leaks</p>
                    <br />
                    <p>Predict "never leaks memory" ~= 0.9 accuracy, but F1 = 0</p>
                    <p>Our algorithm ~= 0.9 accuracy and F1 ~= 0.9</p>

                    <aside class="notes">
                        We designed an algorithm to detect memory leaks using GC Logs. <br />
                        Had to evaluate several algorithmic choices. <br />
                        Had to prove we're better than a coin. <br />
                        Baseline: either say yes or no to everything. <br />
                        Accounts for inherent bias of an accuracy measure. <br />
                        Better than re-weighting accuracy.  "never leaks memory" would still be 50%, and that's a rubbish algorithm.
                    </aside>
                </section>
                
                <section>
                    <h2>Problem: Reliability of measurement</h2>
                    <!-- TODO: caching in slide. -->
                    <aside class="notes">
                        Common problem in benchmarking <br />
                        Measurement needs to be reliable and trustworthy.
                    </aside>
                </section>
                        
                <section>
                    <h2>Richard's Rule of thumb: If it looks like random noise, it probably is random noise.</h2>
                </section>
                
                <section>
                    <h2>Solution: Check your data</h2>
                    <br />
                    <p>Low standard deviation</p>
                    <p>Coefficient of Variation = Standard Deviation / Mean <p>

                    <aside class="notes">
                        Formalise the rule of thumb. <br />
                        CoV allows you to threshold abstracting away how big your S.D is.
                    </aside>
                </section>

                <section>
                    <h2>Caveat: Non-normal distributons</h2>
                    <img src="imgs/wygh.gif" />

                    <aside class="notes">
                        For non-normal, the standard deviation can be a terrible estimator of scale. <br />
                        For example, in the presence of a single outlier, the standard deviation can grossly overestimate the variability of the data <br />
                        how long people spend reading your blog <br />
                    </aside>
                </section>

                <section>
                    <img src="imgs/black+swan+taleb.jpg" />
                    <aside class="notes">
                        <!-- TODO: summarise -->
                    </aside>
                </section>

                <section>
                    <h2>Solution: Go MAD</h2>
                </section>
                
                <section>
                    <h2>Median Absolute Deviation</h2>
                    <aside class="notes">
                        replace the use of mean with median when calculating s.d. then divide by median. <br />
                        More resiliant to outliers. <br />
                        Used in analysing GC, since S.D. was prone to outlier effects, eg app-server startup times. <br />
                    </aside>
                </section>
                
                <section>
                    <h2>Problem: Experimental Flukes</h2>
                </section>
                
                <section>
                    <h2>Is your A/B testa heisen test?</h2>
                </section>

                <section>
                    <h2>Solution: P-Value</h2>
                    <aside class="notes">
                        Probability that youâ€™ve fluked your way to the result that you got. <br />
                        Threshold at 5% or 1%. <br />
                        formula per distribution <br />
                            where do you lie <br />
                            integrate away from the mean <br />
                    </aside>
                </section>

                <section>
                    <h2>Science Works - B****es!</h2>
                    <aside class="notes">
                        Dawkins or XKCD <br />
                        find the right measure for your problem <br />
                        find a way of having confidence in your measurement
                    </aside>
                </section>

                <!-- TODO: consider colour for section start -->
                <section>
                    <h1>Part 2: Practical Problems</h1>
                </section>

                <section>
                    <h1>Part 3: Applying the Theory</h1>
                </section>

                <section>
                    <h1>Correlation</h1>
                    <h4>A measure of the strength of dependence between two variables</h4>
                    <aside class="notes">
                        Produces a measure that denotes the dependence between variables
                    </aside>
                </section>

                <section>
                    <h1>Pearson Correlation</h1>
                    <img src="imgs/pearsons.png" style="float:left"/>
                    <p>Err...Just look it up</p></br>
                    
                    <p>(Assumes linear relationship)<p/>
                    
                    <aside class="notes">
                        Produces a measure that denotes the dependence between variables
                    </aside>
                </section>

                <section>
                    <table>
                       <tr><th>Range</th><th>Strength</th></tr>
                       <tr><td>&lt;0.4</td><td>Weak/No Correlation</td></tr>
                       <tr><td>&lt;0.7</td><td>Some Correlation</td></tr>
                       <tr><td>&gt;0.7</td><td>Strong Correlation</td></tr>
                    </table>
                    <aside class="notes">
                        Negatives just as important
                    </aside>
                </section>
                <section>
                    <img src="imgs/correlation.png" />
                    <p>Correlation Strength: 0.78453</p>
                    <aside class="notes">
                        Tie back source of system time
                        Disk IO vs system time
                        Correlation strength of 0.78453
                        Generally a strong correlation
                    </aside>
                </section>
                
                
                
                
                
                <section>
                    <h1>Machine Learning</h1>
                    <p>Application of statistics to learn a relationship</p>
                    
                    <aside class="notes">
                        ideas apply elsewhere
                        out of scope to deal with algorithms
                    </aside>
                </section>
                

                <section>
                    <h1>Fitting</h1>
                    <img src="imgs/fitting.png" />

                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <h1>Fitting</h1>
                    <img src="imgs/fitting-new-point.png" />

                    <aside class="notes">
                    </aside>
                </section>
                
                
                <section>
                    <h1>Solution:</h1>
                    <h1>Cross Validation</h1>
                </section>
                
                <section>
                    <img src="imgs/cross-validation.png" />

                    <aside class="notes">
                        Avoid over fitting your training set
                        Hold data back
                        Re-test on held back data - use that as your measure
                        shows your idea generalizes
                    </aside>
                </section>

                <section>
                    <h3>Choose cross validation data wisely</h3>

                    <aside class="notes">
                        Sample cross-validation set
                            Select data randomly
                            selectively, based on domain knowledge
                        Our example: hardware
                            Check we generalise across hardware by adding samples from hardware not in the training set to the CV
                    </aside>
                </section>

                <section>
                    <h3>Self Validating</h3>
                    <p>Enemble methods - Train lots of weak classifiers and average</p>

                    <aside class="notes">
                      self validating
                        some algorithms validate for you
                         random forest+oob
                         only via random sampling
                         all ensemble methods can validate via out of bag
                    </aside>
                </section>

                <section>
                    <h3>Random Forest and Bagging</h3>
                    <p>Divide the data into bootstrap sets</p>
                    <p>Use the rest for calculating error</p>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h3>Monitor Production Data...It changes</h3>
                    <p>Is it still the same data that you learnt with?<p>

                    <aside class="notes">
                        monitor all the things in production if possible
                          input/output
                          distribution with thresholds
                          expert eyeballing
                          stratified sampling
                          manual classification
                          precision/recall
                    </aside>
                </section>

                <section>
                    <h3>A/B Test new systems</h3>
                    <p>Satisfaction/Profit/Traffic...</p>
                    <aside class="notes">
                        monitor validation after retraining
                          A/B Business Metrics
                          Satisfaction
                          Belief
                          Profit
                          Engagement
                          Traffic
                    </aside>
                </section>

                <section>
                    <h1>Learning Curves</h1>
                    <aside class="notes">
                       Bias:
                          The less data you have the easier it is to fit, thus error will be low on the training set.
                          However it will not generalise well so high error on validation set.
                          The more data you get the harder it is to fit the training set thus error on training set increases.
                          However the increased number of samples does help fit the validation set (but poorly)
                          Eventualy cv error ~= training error
            
                       Variance:
                          Fits training very well, adding more samples does not increase error much
                          CV error is high, but does not reduce much hence they are far apart
                    </aside>
                </section>
                <section>
                    <img src="imgs/under-fitting.png" />
                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <img src="imgs/over-fitting.png" />
                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <h2>How much is too much?</h2>
                    <aside class="notes">
                        RF can be tuned to over or under fit by setting the number of nodes in a decision tree.
                        How do you choose that number?
                        Similar approach to learning curves except insted of controlling number of samples restrict the algoriths ability to over fit.
                        Gradualy increase the size of the trees. If they decrease together should not be over fitting.
                        If they diverge then they can fit the training set but it does not generalise to the validation set...probably over fitting.
                    </aside>
                </section>
                <section>
                    <img src="imgs/learning-curve.png" />
                    <aside class="notes">
                        In our case validation error drops with training error
                        Possible bias problem as error remains fairly high however 10% was acceptable for our purposes
                        Improvement over 64 nodes is relatively small so we can save some clock cycles and limit tree sizes to 64.
                        Validation data is also from a different source showing model generalises
                    </aside>
                </section>
                <section>
                    <img src="imgs/learning-curve2.png" />
                    <aside class="notes">
                        Divergence did happen above 2000 nodes
                    </aside>
                </section>
                <section>
                    <img src="imgs/real-curve.png" />
                    <aside class="notes">
                         Apply technique to number of samples. Using 64 nodes in the tree
                         Looks reasonably flat, not converging together.
                         Possible bias but error ok for our purposes
                    </aside>
                </section>
            </div>
        </div>


        <!--
                    @johno_oliver
                    @RichardWarburto
                    http://insightfullogic.com
                    -->

		<script src="reveal.js/lib/js/head.min.js"></script>
		<script src="reveal.js/js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimelreveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>


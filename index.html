<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Better than a Coin Toss</title>

		<meta name="description" content="">
		<meta name="author" content="Richard Warburton">
		<meta name="author" content="John Oliver">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.js/css/reveal.min.css">
		<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		<div class="reveal">
			<div class="slides">

				<section>
					<h1>Are you better than a Coin Toss?</h1>
					<h3>by John Oliver and Richard Warburton</h3>
                    
                    <img height="300px" src="imgs/2275021-two_face.jpg" />

					<aside class="notes">
					</aside>
				</section>

				<section>
                    <img src="imgs/mad_men.jpg" />

					<aside class="notes">
                        <br /> Display Advertising
                        <br /> Complex internal algorithm
                        <br /> Baselined by an expert found to be no better than randomly selecting adverts
					</aside>

				</section>
				
                <section>
                    <h2>Who are we?</h2>

                    <aside class="notes">
                        <br /> John first
                        <br /> Working on Automated Performance Tuning Problems
                        <br /> based on science
                        <br /> ML at work
					</aside>
				</section>
                
                <section>
                    <ul style="font-size: 48px; line-height:2">
                        <li>Why you should care</li>
                        <li>The Fundamentals</li>
                        <li>Practical Problems</li>
                        <li>Applying the Theory</li>
                    </ul>
                    <aside class="notes">
                        define structure of the talk
					</aside>
				</section>

                <section>
                    <img src="imgs/Moneyballsbn.jpg" />
                    <aside class="notes">
                        Sport - pundits are useless sheep.<br />
                        Billy Beane managed Oakland Athletics <br />
                        Bill James developed Sabermetrics <br />
					</aside>
				</section>

                <section>
                    <img src="imgs/tetlock.gif" style="float: left;"/>
                    <img src="imgs/nate-silver.jpg" />
                    
                    <aside class="notes">
                        Politics:
                            2008 and 2012 elections had predictable results.
                            Nate Silver combined individual poll ratings.
                            Dismissed by critics, data proved him right.
					</aside>
				</section>
                
                <section>
                    <img src="imgs/fivethirtyeight-0909-signal1-blog480.png" />
                    <aside class="notes">
                        Improvement over time
                        meterology
                        hurricane
					</aside>
				</section>

                <section>
                    <h2>'Experts" aren't very good</h2>
                    <aside class="notes">
                        Across a wide variety of industries <br />
                        If just a someone's opinion <br />
                        people put too much value in their own predictive power
					</aside>
				</section>

                <section>
                    <img src="imgs/111026_SCI_kahneman.jpg.CROP.rectangle3-large.jpg" />
                    
                    <aside class="notes">
                        <br /> Daniel Kahneman and Amos Tversky
                        <br /> Nobel Prize in Economics Sciences
                        <br /> Prospect Theory - explain poor decisions when risk is a factor
					</aside>
				</section>

                <section>
                    <h1>Big Data solves ALL KNOWN PROBLEMS</h1>
				</section>

                <section>
                    <h1>Big Data <strike>solves ALL KNOWN PROBLEMS</strike></h1>
                    <h3> ... helps </h3>
                    <aside class="notes">
                        <br />recording data not enough
                        <br />need to know how to analyse it
                        <br />New problem: validation
					</aside>
				</section>

                <section>
                    <h1>Validation = Tests for Data</h1>
                    <aside class="notes">
                        explain validation <br />
                        You wouldn’t trust software without testing it - how do you trust your numbers? <br />
                        We've all seen the value of automated testing, CI, TDD in software development <br />
                        big data arena needs to build on that success
					</aside>
				</section>

                <section data-transition="linear" data-background="#007777" data-background-transition="slide">
                    <h1>Part 1: Fundamentals</h1>
                    <aside class="notes">
                        Talk about fundamentals, by which we mean basic statistics. <br />
                        Probably familiar to some, but important to cover fundamentals.
                    </aside>
                </section>
                
                <section>
                    <h2>Null Hypothesis</h2>
                    <p>Until proven otherwise there is no relationship between phenomena</p>
                    <aside class="notes">
                        <p>Refuting The Null Hypothesis is to present evidence that there is a relationship</p>
                        <p>Hypothesis: the loss of my socks is due to alien burglary</p>
                        <p>Null Hypothesis: the loss of my socks is not caused by an alien burglary</p>
                    </aside>
                </section>
               
               <!--
                <section>
                    <h4>Hypothesis: the loss of my socks is due to alien burglary</h4>
                    <h4>Null Hypothesis: the loss of my socks is not caused by an alien burglary</h4>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h4>Refuting The Null Hypothesis is to present evidence that there is a relationship</h4>
                    <aside class="notes">
                    </aside>
                </section>
                -->

                <!-- TODO: improve table formatting -->
                <section>
                    <h3>When you hear "Wolf!" there is a wolf nearby</h3>
                    <br />
                    <table>
                        <tr>
                            <td></td>
                            <th>Cry "Wolf!"</th>
                            <th>Stay Quiet</th>
                        </tr>
                        <tr>
                            <th>Wolf Nearby</th>
                            <td>Ok</td>
                            <td>False Negative</td>
                        </tr>
                        <tr>
                            <th>Its really a chicken!</th>
                            <td>False Positive</td>
                            <td>Ok</td>
                        </tr>
                    </table>
                    <br />
                    <br />
                    <br />
                    <aside class="notes">
                        <h4>False Positives vs False Negatives</h4>
                        FN or type I error - when the null hypothesis (H0) is true, but is rejected <br />
                        FP or type II error - when the null hypothesis (H0) is false, but is accepted
                    </aside>
                </section>

                <section>
                    <h2>Why is this important?</h2>
                    <aside class="notes">
                        FP and FN not equal <br />
                        Depend on business needs and context. <br />
                        Different people value them differently.
                    </aside>
                </section>

                <section>
                    <blockquote>It is better that ten guilty persons escape than that one innocent suffer</blockquote>
                    <p>- William Blackstone</p>
                    <aside class="notes">
                        Innocent until proven guilty <br />
                        The law considers FP more harmful than FN.
                    </aside>
                </section>

                <section>
                    <img src="imgs/sleazy-salesman.jpg" />
                    <aside class="notes">
                        Estate Agents would rather show people the wrong house than miss out on a sale.
                    </aside>
                </section>
               
                <section>
                    <h2>Static Analysis</h2>
                    <img height="128px" src="imgs/eclipse3.2.gif" />
                    <img height="128px" src="imgs/pmd_logo.png" />
                    <img height="128px" src="imgs/buggy-sm.png" />
                    <img height="128px" src="imgs/coverity.jpg" />
                    <aside class="notes">
                        Coverity published research findings that low false positive rates are good for business. <br />
                        If you tell developers there's a bug and there isn't one - they lose confidence in your tooling. <br />
                        But they aren't expecting you to find all the bugs in their code. <br />
                        Bill Pugh - started findbugs sometimes recommends that people run it once on their codebase and then
                        ignore all those warnings.  If its working in production, its likely not an important bug.
                    </aside>
                </section>

                <section>
                    <h2>Cost benefit Analysis</h2>
                    <ul>
                        <li>Costs a lot to jail an innocent man
                        <li>Costs very little to show someone an inappropriate house
                        <li>Credibility, Liberty, Morality are also costs
                    </ul>
                </section>

                <section>
                    <h2>Choose the right measurement</h2>
                    <p>There's more than one concept of accuracy</p>
                    <aside class="notes">
                        Most people think of 'accuracy' <br />
                        Accuracy: what you got right / everything <br />
                        Not an appropriate measurement for all situations.
                    </aside>
                </section>

                <section>
                    <h2>Recall</h2>
                    <p>number of true positives / number of actually true values</p>
                    <aside class="notes">
                        Proportion of true values you've found. <br />
                        How how much did you get right?<br />
                        Low score = too many FNs.<br />
                        Irrational Skeptic (Richard's Dad)
                    </aside>
                </section>

                <section>
                    <h2>Precision</h2>
                    <p>number of true positives / predicted true value</p>
                    <aside class="notes">
                        Proportion of your claimed truths which were true. <br />
                        Low score = too many FPs.<br />
                        Irrational Optimist (Dot Com Boom)
                    </aside>
                </section>

                <section>
                    <h2>F Measure</h2>
                    <img src="imgs/fmeasure.jpg" />
                    <aside class="notes">
                        Weighted tradeoff between precision and recall. <br />
                        We need to have a balance between each. <br />
                        Won't explain formula, go read about it. <br />
                        F1 = even tradeoff between precision and recall.
                    </aside>
                </section>

                <section>
                    <h2>Case Study: Memory Leaks</h2>
                    <br />
                    <p>About ~10% of our dataset had memory leaks</p>
                    <br />
                    <p>Predict "never leaks memory" ~= 0.9 accuracy, but F1 = 0</p>
                    <p>Our algorithm ~= 0.9 accuracy and F1 ~= 0.9</p>

                    <aside class="notes">
                        We designed an algorithm to detect memory leaks using GC Logs. <br />
                        Had to evaluate several algorithmic choices. <br />
                        Had to prove we're better than a coin. <br />
                        Baseline: either say yes or no to everything. <br />
                        Accounts for inherent bias of an accuracy measure. <br />
                        Better than re-weighting accuracy.  "never leaks memory" would still be 50%, and that's a rubbish algorithm.
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: Reliability of measurement</h3>
                    <img src="imgs/Steady-vs-erratic-jmsr.png" />
                    <aside class="notes">
                        Common problem in benchmarking <br />
                        Measurement needs to be reliable and trustworthy.
                    </aside>
                </section>
                        
                <section>
                    <h3>Rule of thumb</h3>
                    <p>If it looks like random noise, it probably is random noise.</p>
                </section>
                
                <section>
                    <h3>Solution: Check your data</h3>
                    <br />
                    <p>Low Standard Deviation</p>
                    <img src="imgs/standard-deviation.jpg" />
                    <p>Coefficient of Variation = Standard Deviation / Mean <p>

                    <aside class="notes">
                        Formalise the rule of thumb. <br />
                        CoV allows you to threshold abstracting away how big your S.D is.
                    </aside>
                </section>

                <section>
                    <h3>Caveat: Non-normal distributons</h3>
                    <img src="imgs/wygh.jpg" />

                    <aside class="notes">
                        For non-normal, the standard deviation can be a terrible estimator of scale. <br />
                        For example, in the presence of a single outlier, the standard deviation can grossly overestimate the variability of the data <br />
                        how long people spend reading your blog <br />
                    </aside>
                </section>

                <section>
                    <img src="imgs/black+swan+taleb.jpg" />
                    <aside class="notes">
                        Nassim Nicholas Taleb <br />
                        high-profile rare events are incredibly important <br />
                        hard to predict because they're rare and aren't normally distributed<br />
                        psychological biases make us less aware than we should be.
                    </aside>
                </section>

                <section>
                    <h2>Solution: Go MAD</h2>
                </section>
                
                <section>
                    <h3>Median Absolute Deviation</h3>
                    <img src="imgs/mad.jpg" />
                    <aside class="notes">
                        replace the use of mean with median when calculating s.d. then divide by median. <br />
                        More resiliant to outliers. <br />
                        Used in analysing GC, since S.D. was prone to outlier effects, eg app-server startup times. <br />
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: Experimental Flukes</h3>
                </section>
                
                <section>
                    <h3>Is your A/B test a heisen test?</h3>
                </section>

                <section>
                    <h3>Solution: P-Value</h3>
                    <img src="imgs/P-value_Graph.jpg" />
                    <aside class="notes">
                        Probability that you’ve fluked your way to the result that you got. <br />
                        Threshold at 5% or 1%. <br />
                        formula per distribution <br />
                            google it <br />
                            where do you lie <br />
                            integrate away from the mean <br />
                    </aside>
                </section>

                <section>
                    <h2>Science Works - B****es!</h2>
                    <aside class="notes">
                        Dawkins or XKCD <br />
                        find the right measure for your problem <br />
                        find a way of having confidence in your measurement
                    </aside>
                </section>

                <section data-transition="linear" data-background="#4d7e65" data-background-transition="slide">
                    <h1>Practical Problems</h1>
                    <h3>Part 2</h3>
                    <aside class="notes">
                        Talked a lot about theory - this isn't High School Maths! <br />
                        what gets in the way of applying the theory <br />
                        "Everyone has a plan, until they get punched in the mouth" - Mike Tyson
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: False Prophets</h3>
                    <aside class="notes">
                    </aside>
                </section>

                <section>
                    <h3>I'm an expert, listen to me!</h3>
                    <aside class="notes">
                        Everyone has a pet theory <br />
                        No one wants to admit they’re wrong
                    </aside>
                </section>

                <section>
                    <h3>Solution: Establish Goals and Hypothesis then test solutions</h3>
                    <aside class="notes">
                        Don’t be afraid of failure <br />
                        Don’t try to debate the solution, just set goals and measure outcomes <br />
                        Test things you think won't work <br />
                        Success speaks volumes
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: Code Quality</h3>
                    <aside class="notes">
                        Data Scientists don't <br />
                        bugs in code cause analytic mistakes
                    </aside>
                </section>

                <section>
                    <h3>Growth in a Time of Debt</h3>
                    
                    <aside class="notes">
                      Reinhart and Rogoff</br>
                      Found that 90% debt to GDP ratio is a tipping point for econnomies</br>
                      Amoung other possible errors coding mistakes were found in spreadsheets that undermined the proposal</br>
                      "Rogoff and Reinhardt claimed that their fundamental conclusions were accurate, despite the errors"</br>
                    </aside>
                </section>

                <section>
                    <blockquote>
                        The internal SRI software exception was caused during execution of a data
                        conversion from 64-bit floating point to 16-bit signed integer value. The
                        floating point number which was converted had a value greater than what could
                        be represented by a 16-bit signed integer. This resulted in an Operand Error.
                    </blockquote>
                    <aside class="notes">
                        Flight 501 Failure - Report by the Inquiry Board <br />
                        Data analysis failure, algorithm based on sensor data which fails.
                    </aside>
                </section>
                
                <section>
                    <h3>Solution: Software Engineering Practices</h3>
                    <aside class="notes">
                        Teach your scientists some SE.<br />
                        Write tests <br />
                        Treat your R/scikit scripts like the rest of your codebase.
                    </aside>
                </section>

                <section>
                    <blockquote>Everyone Lies</blockquote>
                    <p>- House</p>
                    <aside class="notes">
                        Humans a big source of input data.<br />
                        Are you asking them, or measuring their behaviour?<br />
                        Need to survey people sometimes <br />
                        opinions not objective <br />
                        maybe not trustworthy
                    </aside>
                </section>

                <section>
                    <h2>Solution: Understand Biases and Design around them</h2>
                </section>

                <section>
                    <blockquote>Gay couples should have an equal right to get married, not just to have civil partnerships</blockquote>
                    <p class="fragment">Populus: 65% vs 27%</p>

                    <br />
                    <br />

                    <blockquote>Marriage should continue to be defined as a life-long exclusive commitment between a man and a woman</blockquote>
                    <p class="fragment">Comres + Catholic Voices: 22% vs 70%</p>
                    <aside class="notes">
                        Leading Questions <br />
                        Opinion polling on gay marriage. <br />
                        Catholic Voices commissioned Comres, Newspapers commissioned Populus. <br />
                    </aside>
                </section>
                
                <section>
                    <h3>Acquiescence Bias</h3>
                    <p>Answer yes if there’s a positive connotation</p>
                </section>

                <section>
                    <h4>Removal of Particular Advertising and Sponsorship Bans </h4>
                    <code>
                        FOR: 1045 <br />
                        AGAINST: 731 <br />
                        ABSTAIN: 121 <br />
                        Motion Carried <br />
                    </code>
                    
                    <br />

                    <h4>Maintaining an Ethical Union by Reaffirming Advertising and Sponsorship Bans</h4>
                    <code>
                        FOR: 858<br />
                        AGAINST: 755<br />
                        ABSTAIN: 166<br />
                        Motion Carried<br />
                    </code>
                    <aside class="notes">
                        Student Politics - ridiculous, love referenda <br />
                        Warwick University Student's Union 2010
                    </aside>
                </section>
                
                <section>
                    <h3>Solution: phrase questions neutrally</h3>
                    <p>And only have one question</p>
                </section>

                <section>
                    <h3>Social Desirability</h3>
                    <p>Poor people overestimate their income, rich people under estimate it.</p>
                </section>

                <section>
                    <h3>Solutions</h3>
                    <ul>
                        <li>Anonymisation
                        <li>Confidentiality
                        <li>Randomized Response
                        <li>Bogus Pipeline
                    </ul>
                    <aside class="notes">
                        Ask a man whether he had sex with a prostitute this month. <br />
                        Before he answers ask him to flip a coin. Instruct him to answer "yes" if the coin comes up tails, and truthfully, if it comes up heads. <br />
                        Only he knows whether his answer reflects the toss of the coin or his true experience. <br />
                        Bogus pipeline - fake polygraph, dna tests, randomised drug testing <br />
                        thanks Trisha Goddard and Jeremy Kyle
                    </aside>
                </section>

                <section>
                    <h3>Bias towards the first answer of a question</h3>
                    <p>Make sure to randomise the order of answers </p>
                </section>

                    <section>
                        <h3>What will the next crisis in Washington be?</h3>
                        <br />
                        <ul>
                            <li>Fight over the debt ceiling
                            <li>Difficulty averting automatic cuts to the Pentagon
                            <li>Failure to pass basic budget bills
                            <li>All of the above
                        </ul>

                        <br />
                        <br />
                        <p><small>http://www.foxnews.com/politics/elections/2012/you-decide/what-will-next-crisis-washington-be</small></p>
                        <aside class="notes">
                            Poorly Chosen answers <br />
                            Here an example of deliberate bias. <br />
                            Easy to do the same in your questioning.
                        </aside>
                    </section>

                    <section>
                        <h3>Problem: Correlation doesn’t imply Causality</h3>
                        <aside class="notes">
                            Post hoc ergo propter hoc
                            what's the real cause?
                        </aside>
                    </section>
                    
                    <section>
                        <h3>Database and network activity correlating</h3>
                        <p>Performance Diagnosis: was actually a GC Problem.</p>
                        <aside class="notes">
                        What is causing what? <br />
                        correlation can be nonsense <br />
                        Common root cause: GC pause affected both Networking and DB threads.
                    </aside>
                </section>

                <section>
                    <h3>Solution: Domain Knowledge</h3>
                    <aside class="notes">
                        use domain knowledge to disambiguate causality <br />
                        understand the mechanics at play and model them. <br />
                        We'll come back to correlation later in the talk.
                    </aside>
                </section>

                <section>
                    <img src="imgs/blog_raf_bullet_holes_0.jpg">
                    <aside class="notes">
                        WWII - Adding armour in the wrong place caused increased fatalities. <br />
                        Abraham Wald's recommendation: <br />
                        adding armour to the places where there were no bullets was the recommended solution <br />
                    </aside>
                </section>
                
                <section>
                    <h3>Solutions</h3>
                    <ul>
                        <li>Use domain knowledge - ask Pilots
                        <li>Stratified sample sets
                        <li>Measure outcomes - are planes surviving more?
                    </ul>

                    <aside class="notes">
                        Divide problems into different classes, called strata. <br />
                        Sample evenly from different strata. <br />
                        Try to find an appropriate number of shot down planes.  Even if its harder.
                    </aside>
                </section>

                <section>
                    <h2>Be Rigorous</h2>
                    <aside class="notes">
                        Good Methods <br />
                        Good Code <br />
                        Good Data
                    </aside>
                </section>

                <section data-transition="linear" data-background="#8c4738" data-background-transition="slide">
                    <h1>Part 3: Applying the Theory</h1>
                    <aside class="notes">
                        Provide some examples of how we've applied Stats + ML at jClarity.
                    </aside>
                </section>

                <section>
                    <h1>Correlation</h1>
                    <h4>A measure of the strength of dependence between two variables</h4>
                    <aside class="notes">
                        Produces a measure that denotes the dependence between variables
                    </aside>
                </section>

                <section>
                    <h1>Pearson Correlation</h1>
                    <img src="imgs/pearsons.png" style="float:left"/>
                    <p>Err...Just look it up</p></br>
                    
                    <p>(Assumes linear relationship)<p/>
                    
                    <aside class="notes">
                        Produces a measure that denotes the dependence between variables <br />
                        other of correlation as well: auto-correlation, spearman's rank
                    </aside>
                </section>

                <section>
                    <table>
                       <tr><th>Range</th><th>Strength</th></tr>
                       <tr><td>&lt;0.4</td><td>Weak/No Correlation</td></tr>
                       <tr><td>&lt;0.7</td><td>Some Correlation</td></tr>
                       <tr><td>&gt;0.7</td><td>Strong Correlation</td></tr>
                    </table>
                    <aside class="notes">
                        Negatives just as important - remember to normalise
                    </aside>
                </section>
                <section>
                    <img src="imgs/correlation.png" />
                <p>Correlation Strength: 0.78453</p>
                <aside class="notes">
                    Explain goal: Tie back source of system time <br />
                    Disk IO vs system time <br />
                    Correlation strength of 0.78453 <br />
                    Generally a strong correlation
                </aside>
            </section>
            
            <section>
                <h1>Machine Learning</h1>
                <p>Application of statistics to learn a relationship</p>
                
                <aside class="notes">
                    ideas apply elsewhere <br />
                    out of scope to deal with algorithms <br />
                </aside>
            </section>
            
            <section>
                <h3>How many clusters?</h3>
                <img src="imgs/Cluster-2.jpg" />
                <aside class="notes">
                    Clustering - grouping sets of similar objects <br />
                    Some clustering algorithms, eg kmeans, have a parameter of number of clusters <br />
                    Tradeoff: smaller clusters = is more grouped together, but less value in grouping.
                </aside>
            </section>

            <section>
                <h3>Where's the elbow?</h3>
                <img src="imgs/DataClustering_ElbowCriterion.JPG" />
                <aside class="notes">
                Explain Hyper Parameter Optimisation <br />
                Show a graph - point out the elbow <br />
                determine some k <br />
                for k in min, min + 1... <br />
                    validation_error(k) / k <br />
                    or validation_error(k) / fn(k) <br />
                    stop if you drop <br />
                    relies on validation being monotonically increasing for k <br />
                    bigger k suggests overfitting
                </aside>
            </section>
            
            <section>
                <h1>Fitting</h1>
                <img src="imgs/fitting.png" />

                <aside class="notes">
                    Explain validation issue for classification problems <br />
                    red vs blue <br />
                </aside>
            </section>
            <section>
                <h1>Fitting</h1>
                <img src="imgs/fitting-new-point.png" />

                <aside class="notes">
                    added new point
                    poor generalisation, but 100% on test set
                </aside>
            </section>
            
            <section>
                <h1>Solution:</h1>
                <h1>Cross Validation</h1>
            </section>
            
            <section>
                <img src="imgs/cross-validation.png" />

                <aside class="notes">
                    Avoid over fitting your training set <br />
                    Hold data back <br />
                    Re-test on held back data - use that as your measure <br />
                    shows your idea generalizes
                </aside>
            </section>

            <section>
                <h3>Choose cross validation data wisely</h3>

                <aside class="notes">
                    Sample cross-validation set <br />
                        Select data randomly <br />
                        selectively, based on domain knowledge <br />
                    Our example: hardware <br />
                        Check we generalise across hardware by adding samples from hardware not in the training set to the CV
                </aside>
            </section>

            <section>
                <h3>Self Validating</h3>
                <p>Ensemble methods - Train lots of weak classifiers and merge</p>

                <aside class="notes">
                  self validating <br />
                    some algorithms validate for you <br />
                     random forest+oob <br />
                     only via random sampling <br />
                     all ensemble methods can validate via out of bag <br />
                </aside>
            </section>

            <section>
                <h3>Random Forest and Bagging</h3>
                <p>Divide the data into bootstrap sets</p>
                <p>Use the rest for calculating error</p>
                <aside class="notes">
                </aside>
            </section>
            
            <section>
                <h1>Learning Curves</h1>
                <aside class="notes">
        
                </aside>
            </section>
            <section>
                <img src="imgs/under-fitting.png" />
                <aside class="notes">
                   Bias: <br />
                      The less data you have the easier it is to fit, thus error will be low on the training set. <br />
                      However it will not generalise well so high error on validation set. <br />
                      The more data you get the harder it is to fit the training set thus error on training set increases. <br />
                      However the increased number of samples does help fit the validation set (but poorly) <br />
                      Eventualy cv error ~= training error
                </aside>
            </section>
            <section>
                <img src="imgs/over-fitting.png" />
                <aside class="notes">
                   Variance: <br />
                      Fits training very well, adding more samples does not increase error much <br />
                      CV error is high, but does not reduce much hence they are far apart <br />
                </aside>
            </section>
            <section>
                <h2>How much is too much?</h2>
                
                <img src="imgs/smallTree.png"/>
                <img src="imgs/tree.png"/>
                <aside class="notes">
                    RF can be tuned to over or under fit by setting the number of nodes in a decision tree. <br />
                    How do you choose that number? <br />
                    Similar approach to learning curves except insted of controlling number of samples restrict the algoriths ability to over fit. <br />
                    Gradualy increase the size of the trees. If they decrease together should not be over fitting. <br />
                    If they diverge then they can fit the training set but it does not generalise to the validation set...probably over fitting.
                </aside>
            </section>
            <section>
                <img src="imgs/learning-curve.png" />
                <aside class="notes">
                    In our case validation error drops with training error <br />
                    Possible bias problem as error remains fairly high however 10% was acceptable for our purposes <br />
                    Improvement over 64 nodes is relatively small so we can save some clock cycles and limit tree sizes to 64. <br />
                    Validation data is also from a different source showing model generalises
                </aside>
            </section>
            
            <section>
                <img src="imgs/learning-curve2.png" />
                <aside class="notes">
                    Divergence did happen above 2000 nodes
                </aside>
            </section>

            <section>
                <img src="imgs/bad-learning-curves.png" />
                <aside class="notes">
                    Ask the audience: what's weird? <br />
                    Cross Validation error is too low - this is actually a bug in one of our R scripts.
                </aside>
            </section>

            <section>
                <img src="imgs/real-curve.png" />
                <aside class="notes">
                     Apply technique to number of samples. Using 64 nodes in the tree <br />
                     Looks reasonably flat, not converging together. <br />
                     Possible bias but error ok for our purposes
                </aside>
            </section>
            
            <section>
                <h3>Monitor Production Data...It changes</h3>
                <p>Does it look like the same data that you learnt with?<p>

                <aside class="notes">
                    monitor all the things in production if possible <br />
                      input/output <br />
                      distribution with thresholds <br />
                      expert eyeballing <br />
                      stratified sampling <br />
                      manual classification <br />
                      precision/recall <br />
                </aside>
            </section>

            <section>
                <h3>A/B Test new systems</h3>
                <p>Satisfaction/Profit/Traffic...</p>
                <aside class="notes">
                    monitor validation after retraining <br />
                      A/B Business Metrics <br />
                      Satisfaction <br />
                      Belief <br />
                      Profit <br />
                      User Engagement <br />
                      Traffic <br />
                </aside>
            </section>
            
            <section>
                <h2>Common Threads</h2>
                <ul>
                    <li>Training set errors are misleading
                    <li>Cross Validation, Production Monitored Values are the ones that really matter
                    <li>Visualise and compare these errors
                </ul>
                <aside class="notes">
                </aside>
            </section>

            <section>
                <h1>Conclusion</h1>
                <ul>
                    <li>Analytics are increasingly important
                    <li>Wide variety of statistical and practical tips to get them right
                    <li>Have fun and Best of luck!
                </ul>
                <aside class="notes">
                </aside>
            </section>

            <section>
                <h1>Questions?</h1>
                @johno_oliver <br />
                @RichardWarburto <br />
                <a href="http://insightfullogic.com">http://insightfullogic.com</a><br />
            </section>

        </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.min.js"></script>

    <script>

        // Full list of configuration options available here:
        // https://github.com/hakimelreveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

            // Optional libraries used to extend on reveal.js
            dependencies: [
                { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
                // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
            ]
        });

    </script>

</body>
</html>


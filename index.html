<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Better than a Coin Toss</title>

		<meta name="description" content="">
		<meta name="author" content="Richard Warburton">
		<meta name="author" content="John Oliver">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.js/css/reveal.min.css">
		<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		<div class="reveal">
			<div class="slides">

				<section>
					<h1>Are you better than a Coin Toss?</h1>
					<h3>by John Oliver and Richard Warburton</h3>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>

				</section>

				<section>
                    <img src="imgs/mad_men.jpg" />

					<aside class="notes">
                        <br /> Display Advertising
                        <br /> Complex internal algorithm
                        <br /> Baselined by an expert found to be no better than randomly selecting adverts
					</aside>

				</section>
				
                <section>
                    <h2>Who are we?</h2>

                    <aside class="notes">
                        <br /> John first
                        <br /> Working on Automated Performance Tuning Problems
                        <br /> based on science
                        <br /> ML at work
					</aside>

				</section>
                
                <section>
                    <ul style="font-size: 48px; line-height:2">
                        <li>Why you should care</li>
                        <li>The Fundamentals</li>
                        <li>Practical Problems</li>
                        <li>Applying the Theory</li>
                    </ul>
				</section>

                <section>
                    <h2>'Experts" aren't very good</h2>
                    
                    <img src="imgs/Daniel_KAHNEMAN.jpg" />
                    
                    <aside class="notes">
                        <br /> Kahneman
                        <br /> Nobel Prize Winner
                        <br /> Prospect Theory - explain poor decisions when risk is a factor
					</aside>

				</section>
                
                <section>
                    <aside class="notes">
                        Sports
					</aside>
				</section>
                
                <section>
                    <aside class="notes">
                        meterology
                        hurricane
                        http://fivethirtyeight.blogs.nytimes.com/2012/09/09/why-weather-forecasters-are-role-models/
					</aside>
				</section>
                
                <section>
                    <aside class="notes">
                        finance
					</aside>
				</section>
                
                <section>

                    <img src="imgs/tetlock.gif" style="float: left;"/>
                    <img src="imgs/nate-silver.jpg" />
                    
                    <aside class="notes">
                        Politics:
                            2008 and 2012 elections had predictable results.
                            Nate Silver combined individual poll ratings.
                            Dismissed by critics, data proved him right.
					</aside>
				</section>

                <section>
                    <h1>Big Data solves ALL KNOWN PROBLEMS</h1>
				</section>

                <section>
                    <h1>Big Data <strike>solves ALL KNOWN PROBLEMS</strike></h1>
                    <h3> ... helps </h3>
                    <aside class="notes">
                        <br />recording data not enough
                        <br />need to know how to analyse it
                        <br />New problem: validation
					</aside>
				</section>

                <section>
                    <h1>Validation = Tests for Data</h1>
                    <aside class="notes">
                        TODO: pithy line/quote
                        explain validation
                        You wouldnâ€™t trust software without testing it - how do you trust your numbers?
					</aside>
				</section>

                <section>
                    <aside class="notes">
                    </aside>
                </section>
		
                <section>
                    <h1>Part 1: Fundamentals</h1>
                    <aside class="notes">
                        Talk about fundamentals, by which we mean basic statistics. <br />
                        Probably familiar to some, but important to cover fundamentals.
                    </aside>
                </section>
                
                <section>
                    <h2>Null Hypothesis</h2>
                    <p>Until proven otherwise there is no relationship between phenomena</p>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h4>Hypothesis: the loss of my socks is due to alien burglary</h4>
                    <h4>Null Hypothesis: the loss of my socks is nothing to do with alien burglary</h4>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h4>Refuting The Null Hypothesis is to present evidence that there is a relationship</h4>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h3>When you hear "Wolf!" there is a wolf nearby</h3>
                    <br />
                    <table>
                        <tr>
                            <td></td>
                            <th>Cry "Wolf!"</th>
                            <th>Stay Quiet</th>
                        </tr>
                        <tr>
                            <th>Wolf Nearby</th>
                            <td>Ok</td>
                            <td>False Negative</td>
                        </tr>
                        <tr>
                            <th>Its really a chicken!</th>
                            <td>False Positive</td>
                            <td>Ok</td>
                        </tr>
                    </table>
                    <br />
                    <br />
                    <br />
                    <aside class="notes">
                        <h4>False Positives vs False Negatives</h4>
                        FN or type I error - when the null hypothesis (H0) is true, but is rejected <br />
                        FP or type II error - when the null hypothesis (H0) is false, but is accepted
                    </aside>
                </section>

                <section>
                    <h2>Why is this important?</h2>
                    <aside class="notes">
                        FP and FN not equal <br />
                        Depend on business needs and context. <br />
                        Different people value them differently.
                    </aside>
                </section>

                <section>
                    <blockquote>It is better that ten guilty persons escape than that one innocent suffer</blockquote>
                    <p>- William Blackstone</p>
                    <aside class="notes">
                        Innocent until proven guilty <br />
                        The law considers FP more harmful than FN.
                    </aside>
                </section>

                <section>
                    <img src="imgs/sleazy-salesman.jpg" />
                    <aside class="notes">
                        Estate Agents would rather show people the wrong house than miss out on a sale.
                    </aside>
                </section>
               
               <!-- TODO: more logs -->
                <section>
                    <h2>Static Analysis</h2>
                    <img src="imgs/buggy-sm.png" />
                    <img src="imgs/eclipse3.2.gif" />
                    <aside class="notes">
                        Coverity published research findings that low false positive rates are good for business. <br />
                        If you tell developers there's a bug and there isn't one - they lose confidence in your tooling.
                    </aside>
                </section>

                <section>
                    <h2>Cost benefit Analysis</h2>
                    <ul>
                        <li>Costs a lot to jail an innocent man
                        <li>Costs very little to show someone an inappropriate house
                        <li>Credibility, Liberty, Morality are also costs
                    </ul>
                </section>

                <section>
                    <h2>Choose the right measurement</h2>
                    <aside class="notes">
                        Most people think of 'accuracy' <br />
                        Accuracy: what you got right / everything <br />
                        Not an appropriate measurement for all situations.
                    </aside>
                </section>

                <section>
                    <h2>Recall</h2>
                    <p>number of true positives / number of actually true values</p>
                    <aside class="notes">
                        Proportion of true values you've found. <br />
                        How how much did you get right?<br />
                        Low score = too many FNs.<br />
                        Irrational Skeptic (Richard's Dad)
                    </aside>
                </section>

                <section>
                    <h2>Precision</h2>
                    <p>number of true positives / predicted true value</p>
                    <aside class="notes">
                        Proportion of your claimed truths which were true. <br />
                        Low score = too many FPs.<br />
                        Irrational Optimist (Dot Com Boom)
                    </aside>
                </section>

                <section>
                    <h2>F Measure</h2>
                    <!-- TODO: fix image -->
                    <img src="imgs/fmeasure.png" />
                    <aside class="notes">
                        Weighted tradeoff between precision and recall. <br />
                        We need to have a balance between each. <br />
                        Won't explain formula, go read about it. <br />
                        F1 = even tradeoff between precision and recall.
                    </aside>
                </section>

                <section>
                    <h2>Case Study: Memory Leaks</h2>
                    <br />
                    <p>About ~10% of our dataset had memory leaks</p>
                    <br />
                    <p>Predict "never leaks memory" ~= 0.9 accuracy, but F1 = 0</p>
                    <p>Our algorithm ~= 0.9 accuracy and F1 ~= 0.9</p>

                    <aside class="notes">
                        We designed an algorithm to detect memory leaks using GC Logs. <br />
                        Had to evaluate several algorithmic choices. <br />
                        Had to prove we're better than a coin. <br />
                        Baseline: either say yes or no to everything. <br />
                        Accounts for inherent bias of an accuracy measure. <br />
                        Better than re-weighting accuracy.  "never leaks memory" would still be 50%, and that's a rubbish algorithm.
                    </aside>
                </section>
                
                <section>
                    <h2>Problem: Reliability of measurement</h2>
                    <!-- TODO: caching in slide. -->
                    <aside class="notes">
                        Common problem in benchmarking <br />
                        Measurement needs to be reliable and trustworthy.
                    </aside>
                </section>
                        
                <section>
                    <h2>Richard's Rule of thumb: If it looks like random noise, it probably is random noise.</h2>
                </section>
                
                <section>
                    <h2>Solution: Check your data</h2>
                    <br />
                    <p>Low standard deviation</p>
                    <p>Coefficient of Variation = Standard Deviation / Mean <p>

                    <aside class="notes">
                        Formalise the rule of thumb. <br />
                        CoV allows you to threshold abstracting away how big your S.D is.
                    </aside>
                </section>

                <section>
                    <h2>Caveat: Non-normal distributons</h2>
                    <img src="imgs/wygh.gif" />

                    <aside class="notes">
                        For non-normal, the standard deviation can be a terrible estimator of scale. <br />
                        For example, in the presence of a single outlier, the standard deviation can grossly overestimate the variability of the data <br />
                        how long people spend reading your blog <br />
                    </aside>
                </section>

                <section>
                    <img src="imgs/black+swan+taleb.jpg" />
                    <aside class="notes">
                        <!-- TODO: summarise -->
                    </aside>
                </section>

                <section>
                    <h2>Solution: Go MAD</h2>
                </section>
                
                <section>
                    <h2>Median Absolute Deviation</h2>
                    <aside class="notes">
                        replace the use of mean with median when calculating s.d. then divide by median. <br />
                        More resiliant to outliers. <br />
                        Used in analysing GC, since S.D. was prone to outlier effects, eg app-server startup times. <br />
                    </aside>
                </section>
                
                <section>
                    <h2>Problem: Experimental Flukes</h2>
                </section>
                
                <section>
                    <h2>Is your A/B testa heisen test?</h2>
                </section>

                <section>
                    <h2>Solution: P-Value</h2>
                    <aside class="notes">
                        Probability that youâ€™ve fluked your way to the result that you got. <br />
                        Threshold at 5% or 1%. <br />
                        formula per distribution <br />
                            where do you lie <br />
                            integrate away from the mean <br />
                    </aside>
                </section>

                <section>
                    <h2>Science Works - B****es!</h2>
                    <aside class="notes">
                        Dawkins or XKCD <br />
                        find the right measure for your problem <br />
                        find a way of having confidence in your measurement
                    </aside>
                </section>

                <!-- TODO: consider colour for section start -->
                <section>
                    <h1>Part 2: Practical Problems</h1>
                    <aside class="notes">
                        Talked a lot about theory - this isn't High School Maths! <br />
                        what gets in the way of applying the theory
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: False Prophets</h3>
                    <aside class="notes">
                    </aside>
                </section>

                <section>
                    <h3>I'm an expert, listen to me!</h3>
                    <aside class="notes">
                        Everyone has a pet theory <br />
                        No one wants to admit theyâ€™re wrong
                    </aside>
                </section>

                <section>
                    <h3>Solution: Establish Goals and Hypothesis then test solutions</h3>
                    <aside class="notes">
                        Donâ€™t be afraid of failure <br />
                        Donâ€™t try to debate the solution, just set goals and measure outcomes <br />
                        Test things you think won't work <br />
                        Success speaks volumes
                    </aside>
                </section>
                
                <section>
                    <h3>Problem: Code Quality</h3>
                    <aside class="notes">
                        Data Scientists don't <br />
                        Ariane Rocket <br />
                        bugs in code cause analytic mistakes
                    </aside>
                </section>

                <section>
                    <img src="imgs/Ariane-5-Flight-501.jpg" />
                </section>

                <section>
                    <blockquote>
                        The internal SRI software exception was caused during execution of a data
                        conversion from 64-bit floating point to 16-bit signed integer value. The
                        floating point number which was converted had a value greater than what could
                        be represented by a 16-bit signed integer. This resulted in an Operand Error.
                    </blockquote>
                    <aside class="notes">
                        Flight 501 Failure - Report by the Inquiry Board <br />
                        Data analysis failure, algorithm based on sensor data which fails.
                    </aside>
                </section>
                
                <section>
                    <h3>Solution: Software Engineering Practices</h3>
                    <aside class="notes">
                        Teach your scientists some SE.<br />
                        Write tests <br />
                        Treat your R/scikit scripts like the rest of your codebase.
                    </aside>
                </section>

                <section>
                    <blockquote>Everyone Lies</blockquote>
                    <p>- House</p>
                    <aside class="notes">
                        Humans a big source of input data.<br />
                        Are you asking them, or measuring their behaviour?<br />
                        Need to survey people sometimes <br />
                        opinions not objective <br />
                        maybe not trustworthy
                    </aside>
                </section>

                <section>
                    <h2>Solution: Understand Biases and Design around them</h2>
                </section>

                <section>
                    <blockquote>Gay couples should have an equal right to get married, not just to have civil partnerships</blockquote>
                    <p class="fragment">Populus: 65% vs 27%</p>

                    <br />
                    <br />

                    <blockquote>Marriage should continue to be defined as a life-long exclusive commitment between a man and a woman</blockquote>
                    <p class="fragment">Comres + Catholic Voices: 22% vs 70%</p>
                    <aside class="notes">
                        Leading Questions <br />
                        Opinion polling on gay marriage. <br />
                        Catholic Voices commissioned Comres, Newspapers commissioned Populus. <br />
                    </aside>
                </section>

            <!-- TODO: examples
                Social Desirability
                Solution: Anonymisation
                
                Acquiescence
                Answer yes if thereâ€™s a positive connotation
                Solution: phrase questions neutrally
                
                Bias towards the first answer of a question
                Solution: Randomise the order
            --> 

                <section>
                    <h3>What will the next crisis in Washington be?</h3>
                    <br />
                    <ul>
                        <li>Fight over the debt ceiling
                        <li>Difficulty averting automatic cuts to the Pentagon
                        <li>Failure to pass basic budget bills
                        <li>All of the above
                    </ul>

                    <br />
                    <br />
                    <p><small>http://www.foxnews.com/politics/elections/2012/you-decide/what-will-next-crisis-washington-be</small></p>
                    <aside class="notes">
                        Poorly Chosen answers <br />
                        Here an example of deliberate bias. <br />
                        Easy to do the same in your questioning.
                    </aside>
                </section>

                <section>
                    <h3>Problem: Correlation doesnâ€™t imply Causality</h3>
                    <aside class="notes">
                        Post hoc ergo propter hoc
                        what's the real cause?
                    </aside>
                </section>
                
                <section>
                    <h3>Database and network activity correlating</h3>
                    <aside class="notes">
                        What is causing what? <br />
                        correlation can be nonsense
                    </aside>
                </section>

                <section>
                    <h3>Solution: Domain Knowledge</h3>
                    <aside class="notes">
                        use domain knowledge to disambiguate causality <br />
                        Apply performance analysis: look at GC logs and metric data.<br />
                        turns out the example is a gc problem, pauses fucking everything
                    </aside>
                </section>
                
                <section>
                    <img src="imgs/blog_raf_bullet_holes_0.jpg">
                    <aside class="notes">
                        WWII - Adding armour in the wrong place caused increased fatalities. <br />
                        Abraham Wald's recommendation: <br />
                        adding armour to the places where there were no bullets was the recommended solution <br />
                    </aside>
                </section>
                
                <section>
                    <h3>Solutions</h3>
                    <ul>
                        <li>Use domain knowledge - ask Pilots
                        <li>Stratified sample sets
                        <li>Measure outcomes - are planes surviving more?
                    </ul>

                    <aside class="notes">
                        Divide problems into different classes, called strata. <br />
                        Sample evenly from different strata. <br />
                        Try to find an appropriate number of shot down planes.  Even if its harder.
                    </aside>
                </section>

                <section>
                    <h2>Be Rigorous</h2>
                    <aside class="notes">
                        Good Methods <br />
                        Good Code <br />
                        Good Data
                    </aside>
                </section>

                    <section>
                        <h1>Part 3: Applying the Theory</h1>
                    </section>

                    <section>
                        <h1>Correlation</h1>
                        <h4>A measure of the strength of dependence between two variables</h4>
                        <aside class="notes">
                            Produces a measure that denotes the dependence between variables
                        </aside>
                    </section>

                    <section>
                        <h1>Pearson Correlation</h1>
                        <img src="imgs/pearsons.png" style="float:left"/>
                        <p>Err...Just look it up</p></br>
                        
                        <p>(Assumes linear relationship)<p/>
                        
                        <aside class="notes">
                            Produces a measure that denotes the dependence between variables
                        </aside>
                    </section>

                    <section>
                        <table>
                           <tr><th>Range</th><th>Strength</th></tr>
                           <tr><td>&lt;0.4</td><td>Weak/No Correlation</td></tr>
                           <tr><td>&lt;0.7</td><td>Some Correlation</td></tr>
                           <tr><td>&gt;0.7</td><td>Strong Correlation</td></tr>
                        </table>
                        <aside class="notes">
                            Negatives just as important
                        </aside>
                    </section>
                    <section>
                        <img src="imgs/correlation.png" />
                    <p>Correlation Strength: 0.78453</p>
                    <aside class="notes">
                        Tie back source of system time <br />
                        Disk IO vs system time <br />
                        Correlation strength of 0.78453 <br />
                        Generally a strong correlation
                    </aside>
                </section>
                
                
                
                
                
                <section>
                    <h1>Machine Learning</h1>
                    <p>Application of statistics to learn a relationship</p>
                    
                    <aside class="notes">
                        ideas apply elsewhere <br />
                        out of scope to deal with algorithms
                    </aside>
                </section>
                

                <section>
                    <h1>Fitting</h1>
                    <img src="imgs/fitting.png" />

                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <h1>Fitting</h1>
                    <img src="imgs/fitting-new-point.png" />

                    <aside class="notes">
                    </aside>
                </section>
                
                
                <section>
                    <h1>Solution:</h1>
                    <h1>Cross Validation</h1>
                </section>
                
                <section>
                    <img src="imgs/cross-validation.png" />

                    <aside class="notes">
                        Avoid over fitting your training set <br />
                        Hold data back <br />
                        Re-test on held back data - use that as your measure <br />
                        shows your idea generalizes
                    </aside>
                </section>

                <section>
                    <h3>Choose cross validation data wisely</h3>

                    <aside class="notes">
                        Sample cross-validation set <br />
                            Select data randomly <br />
                            selectively, based on domain knowledge <br />
                        Our example: hardware <br />
                            Check we generalise across hardware by adding samples from hardware not in the training set to the CV
                    </aside>
                </section>

                <section>
                    <h3>Self Validating</h3>
                    <p>Ensemble methods - Train lots of weak classifiers and merge</p>

                    <aside class="notes">
                      self validating <br />
                        some algorithms validate for you <br />
                         random forest+oob <br />
                         only via random sampling <br />
                         all ensemble methods can validate via out of bag <br />
                    </aside>
                </section>

                <section>
                    <h3>Random Forest and Bagging</h3>
                    <p>Divide the data into bootstrap sets</p>
                    <p>Use the rest for calculating error</p>
                    <aside class="notes">
                    </aside>
                </section>
                
                <section>
                    <h3>Monitor Production Data...It changes</h3>
                    <p>Is it still the same data that you learnt with?<p>

                    <aside class="notes">
                        monitor all the things in production if possible
                          input/output
                          distribution with thresholds
                          expert eyeballing
                          stratified sampling
                          manual classification
                          precision/recall
                    </aside>
                </section>

                <section>
                    <h3>A/B Test new systems</h3>
                    <p>Satisfaction/Profit/Traffic...</p>
                    <aside class="notes">
                        monitor validation after retraining
                          A/B Business Metrics
                          Satisfaction
                          Belief
                          Profit
                          Engagement
                          Traffic
                    </aside>
                </section>

                <section>
                    <h1>Learning Curves</h1>
                    <aside class="notes">
                       Bias: <br />
                          The less data you have the easier it is to fit, thus error will be low on the training set. <br />
                          However it will not generalise well so high error on validation set. <br />
                          The more data you get the harder it is to fit the training set thus error on training set increases. <br />
                          However the increased number of samples does help fit the validation set (but poorly) <br />
                          Eventualy cv error ~= training error
            
                       Variance: <br />
                          Fits training very well, adding more samples does not increase error much <br />
                          CV error is high, but does not reduce much hence they are far apart <br />
                    </aside>
                </section>
                <section>
                    <img src="imgs/under-fitting.png" />
                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <img src="imgs/over-fitting.png" />
                    <aside class="notes">
                    </aside>
                </section>
                <section>
                    <h2>How much is too much?</h2>
                    <aside class="notes">
                        RF can be tuned to over or under fit by setting the number of nodes in a decision tree. <br />
                        How do you choose that number? <br />
                        Similar approach to learning curves except insted of controlling number of samples restrict the algoriths ability to over fit. <br />
                        Gradualy increase the size of the trees. If they decrease together should not be over fitting. <br />
                        If they diverge then they can fit the training set but it does not generalise to the validation set...probably over fitting.
                    </aside>
                </section>
                <section>
                    <img src="imgs/learning-curve.png" />
                    <aside class="notes">
                        In our case validation error drops with training error <br />
                        Possible bias problem as error remains fairly high however 10% was acceptable for our purposes <br />
                        Improvement over 64 nodes is relatively small so we can save some clock cycles and limit tree sizes to 64. <br />
                        Validation data is also from a different source showing model generalises
                    </aside>
                </section>
                <section>
                    <img src="imgs/learning-curve2.png" />
                    <aside class="notes">
                        Divergence did happen above 2000 nodes
                    </aside>
                </section>
                <section>
                    <img src="imgs/real-curve.png" />
                    <aside class="notes">
                         Apply technique to number of samples. Using 64 nodes in the tree <br />
                         Looks reasonably flat, not converging together. <br />
                         Possible bias but error ok for our purposes
                    </aside>
                </section>
            </div>
        </div>


        <!--
                    @johno_oliver
                    @RichardWarburto
                    http://insightfullogic.com
                    -->

		<script src="reveal.js/lib/js/head.min.js"></script>
		<script src="reveal.js/js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimelreveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>

